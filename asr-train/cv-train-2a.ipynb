{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"]=\"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allen/Documents/Wav2vec_Project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/allen/Documents/Wav2vec_Project/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"train[:100]\", trust_remote_code=True)\n",
    "metadata_file = \"/Users/allen/Documents/Wav2vec_Project/asr/common_voice/cv-valid-train-downsampled.csv\"\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"/Users/allen/Documents/Wav2vec_Project/asr/common_voice/wav-files/\", drop_metadata=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "metadata= pd.read_csv(metadata_file)\n",
    "metadata['filename'] = metadata['filename'].str.replace(\n",
    "    'cv-valid-train/', ''\n",
    ").str.replace(\n",
    "    '.mp3', '.wav'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                               text  \\\n",
      "0    sample-182715.wav  the fine manufacturing company was a bookkeepe...   \n",
      "1    sample-114956.wav  the mixture took on a reddish color almost the...   \n",
      "2    sample-139880.wav                                enough said the boy   \n",
      "3    sample-034538.wav  then he sat in the sunfilled doorway smoking t...   \n",
      "4    sample-142907.wav  i've just guaranteed the bank sufficient funds...   \n",
      "..                 ...                                                ...   \n",
      "191  sample-042142.wav                                    you have no car   \n",
      "192  sample-110037.wav  they looked for the fallen mass but found nothing   \n",
      "193  sample-101888.wav     his way isn't the same as mine nor mine as his   \n",
      "194  sample-135316.wav                             he wasn't an alchemist   \n",
      "195  sample-034446.wav                              now look what you did   \n",
      "\n",
      "     up_votes  down_votes       age gender   accent  duration  \n",
      "0           2           0       NaN    NaN      NaN       NaN  \n",
      "1           1           0  fourties   male  england       NaN  \n",
      "2           1           0       NaN    NaN      NaN       NaN  \n",
      "3           2           1       NaN    NaN      NaN       NaN  \n",
      "4           1           0       NaN    NaN      NaN       NaN  \n",
      "..        ...         ...       ...    ...      ...       ...  \n",
      "191         2           0     teens   male       us       NaN  \n",
      "192         5           3       NaN    NaN      NaN       NaN  \n",
      "193         2           0       NaN    NaN      NaN       NaN  \n",
      "194        11           0       NaN    NaN      NaN       NaN  \n",
      "195         2           0       NaN    NaN      NaN       NaN  \n",
      "\n",
      "[196 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print (metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_transcription(example):\n",
    "    # Extract just the filename from the audio path\n",
    "    audio_filename = example[\"audio\"][\"path\"].split(\"/\")[-1]\n",
    "    \n",
    "    # Find the matching transcription in the metadata\n",
    "    text_row = metadata[metadata[\"filename\"] == audio_filename]\n",
    "    print (audio_filename,text_row)\n",
    "    # Return the transcription if found, else return None\n",
    "    return {\"text\": text_row[\"text\"].iloc[0] if not text_row.empty else None,\"filepath\": example[\"audio\"][\"path\"] if not text_row.empty else None}\n",
    "dataset = dataset.map(match_transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        0.00000000e+00, -3.05175781e-05,  0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05,  0.00000000e+00,  0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05, -3.05175781e-05, -3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00012207,\n",
      "        0.        , -0.00012207]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05, -3.05175781e-05, -3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -1.22070312e-04,  6.10351562e-05,  0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        0.00000000e+00, -3.05175781e-05, -3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        0.00000000e+00,  0.00000000e+00, -3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05, -3.05175781e-05,  3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       3.05175781e-05, 0.00000000e+00, 0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        1.22070312e-04, -1.22070312e-04, -3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -6.10351562e-05, -3.05175781e-05, -2.13623047e-04]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       6.10351562e-05, 0.00000000e+00, 0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -6.10351562e-05,  3.05175781e-05,  0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00021362,\n",
      "       -0.00015259,  0.00012207]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       9.15527344e-05, 0.00000000e+00, 6.10351562e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05, -3.05175781e-05,  0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 3.05175781e-05, 0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        0.00000000e+00,  0.00000000e+00, -3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -1.52587891e-04, -9.15527344e-05, -9.15527344e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        3.05175781e-05,  6.10351562e-05, -3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05, -3.05175781e-05,  0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00042725,\n",
      "        0.00015259, -0.00042725]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       3.05175781e-05, 3.05175781e-05, 6.10351562e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        5.18798828e-04, -6.10351562e-05, -2.13623047e-04]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05,  0.00000000e+00,  3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05,  0.00000000e+00,  0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       3.05175781e-05, 3.05175781e-05, 0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00042725,\n",
      "        0.00030518,  0.00033569]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        1.52587891e-04,  3.05175781e-05, -9.15527344e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05,  0.00000000e+00,  0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05, -3.05175781e-05,  0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 3.05175781e-05, 3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        0.00000000e+00, -3.05175781e-05,  0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        0.00000000e+00,  3.05175781e-05, -6.10351562e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 3.05175781e-05, 0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       3.05175781e-05, 0.00000000e+00, 0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05,  6.10351562e-05, -3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        0.00000000e+00, -3.05175781e-05,  0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.        ,  0.        ,  0.        , ...,  0.00045776,\n",
      "       -0.00112915,  0.00228882]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       5.49316406e-04, 3.05175781e-05, 1.22070312e-04]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        0.00000000e+00, -3.05175781e-05,  3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 3.05175781e-05, 3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       5.49316406e-04, 0.00000000e+00, 9.15527344e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.        ,  0.        ,  0.        , ..., -0.0017395 ,\n",
      "        0.00079346,  0.00079346]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05,  1.22070312e-04,  6.10351562e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       6.10351562e-05, 6.10351562e-05, 0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -1.22070312e-04,  0.00000000e+00,  6.10351562e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       6.10351562e-05, 1.83105469e-04, 2.13623047e-04]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00015259,\n",
      "       -0.00033569, -0.00015259]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 9.15527344e-05, 1.22070312e-04]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        5.79833984e-04, -6.10351562e-05, -1.83105469e-04]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05,  0.00000000e+00,  3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       3.05175781e-05, 3.05175781e-05, 0.00000000e+00]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05, -3.05175781e-05,  6.10351562e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -3.05175781e-05,  0.00000000e+00,  3.05175781e-05]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}]\n"
     ]
    }
   ],
   "source": [
    "# print (dataset['train']['audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['audio', 'text', 'filepath']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    sample-182715.wav\n",
      "1    sample-114956.wav\n",
      "2    sample-139880.wav\n",
      "3    sample-034538.wav\n",
      "4    sample-142907.wav\n",
      "Name: filename, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(metadata[\"filename\"].head())  # Print the first few filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the visual acuity is astonishing', 'how strange africa is thought the boy', 'until then he had considered the omens to be things of this world', 'i had seen all that it would presently bring me', 'my tax dollars pay for those public school proms']\n"
     ]
    }
   ],
   "source": [
    "print (dataset['train']['text'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "def preprocess(batch):\n",
    "    batch[\"input_values\"] = batch[\"audio\"][\"array\"]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(preprocess, remove_columns=[\"audio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allen/Documents/Wav2vec_Project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def prepare_dataset(batch):\n",
    "    # Process the audio input\n",
    "    audio = processor.feature_extractor(\n",
    "        batch[\"input_values\"], \n",
    "        sampling_rate=16_000,\n",
    "        return_attention_mask=False  # Explicitly set this argument for clarity\n",
    "    )\n",
    "    batch[\"input_values\"] = audio.input_values[0]\n",
    "\n",
    "    # Process the text target\n",
    "    with processor.as_target_processor():\n",
    "        text = processor.tokenizer(\n",
    "            batch[\"text\"], \n",
    "            return_attention_mask=False,  # Ensure this doesn't cause conflict\n",
    "            truncation=True  # Prevent token overflow\n",
    "        )\n",
    "        batch[\"labels\"] = text.input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset['train'].train_test_split(test_size=0.3)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Set the device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Move the model and inputs to CPU\n",
    "model.to(device)\n",
    "# processor.to(device)\n",
    "\n",
    "class CustomDataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features = [torch.tensor(feature[\"input_values\"]) for feature in features]\n",
    "        label_features = [torch.tensor(feature[\"labels\"]) for feature in features]\n",
    "\n",
    "        input_features_padded = pad_sequence(input_features, batch_first=True, padding_value=self.processor.feature_extractor.padding_value)\n",
    "        labels_padded = pad_sequence(label_features, batch_first=True, padding_value=-100)\n",
    "        attention_masks = torch.zeros_like(input_features_padded).masked_fill(input_features_padded != self.processor.feature_extractor.padding_value, 1)\n",
    "\n",
    "        return {\n",
    "            \"input_values\": input_features_padded,\n",
    "            \"labels\": labels_padded,\n",
    "            \"attention_mask\": attention_masks\n",
    "        }\n",
    "\n",
    "data_collator = CustomDataCollatorCTCWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = jiwer.wer(label_str, pred_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allen/Documents/Wav2vec_Project/.venv/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,  # Use bf16 instead of fp16\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p8/j65kmwm10d3d5lgjzjydyntm0000gn/T/ipykernel_41892/1212053796.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]/Users/allen/Documents/Wav2vec_Project/.venv/lib/python3.9/site-packages/torch/nn/functional.py:2687: UserWarning: The operator 'aten::_ctc_loss' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  return torch.ctc_loss(\n",
      "                                                  \n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [42:11<1:10:54, 128.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5898.0234375, 'eval_wer': 1.021015761821366, 'eval_runtime': 49.726, 'eval_samples_per_second': 1.187, 'eval_steps_per_second': 0.302, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [44:16<01:00,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5526.326171875, 'eval_wer': 1.021015761821366, 'eval_runtime': 4.8057, 'eval_samples_per_second': 12.277, 'eval_steps_per_second': 3.121, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [45:44<00:00,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5049.53662109375, 'eval_wer': 1.021015761821366, 'eval_runtime': 4.9566, 'eval_samples_per_second': 11.903, 'eval_steps_per_second': 3.026, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [45:48<00:00, 53.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2748.0998, 'train_samples_per_second': 0.15, 'train_steps_per_second': 0.019, 'train_loss': 8511.229166666666, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=51, training_loss=8511.229166666666, metrics={'train_runtime': 2748.0998, 'train_samples_per_second': 0.15, 'train_steps_per_second': 0.019, 'total_flos': 7.741184044157338e+16, 'train_loss': 8511.229166666666, 'epoch': 2.857142857142857})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.52it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5049.53662109375, 'eval_wer': 1.021015761821366, 'eval_runtime': 4.727, 'eval_samples_per_second': 12.482, 'eval_steps_per_second': 3.173, 'epoch': 2.857142857142857}\n"
     ]
    }
   ],
   "source": [
    "print (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./wav2vec2-large-960h-cv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually Test the Model\n",
    "# Load the model and processor (tokenizer) from the saved directory\n",
    "test_model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-large-960h-cv\")\n",
    "test_processor = Wav2Vec2Processor.from_pretrained(\"./wav2vec2-large-960h-cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
